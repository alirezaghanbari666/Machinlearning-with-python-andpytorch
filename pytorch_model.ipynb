{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_model.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install onnxruntime"
      ],
      "metadata": {
        "id": "awtPZozvM4Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import spacy # install command: pip install spacy\n",
        "import string\n",
        "import nltk # install command: pip install nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import onnxruntime # install command: pip install onnxruntime\n",
        "import torch.onnx as onnx"
      ],
      "metadata": {
        "id": "ZbeUvKJ7t8is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python3 -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "gGuvtxBQNBXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "XjN-8GXWNnub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "rfRfRvA8P4Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'max_features': 1000,\n",
        "    'num_epochs':200,\n",
        "    'learning_rate':1e-1,\n",
        "    'batch_size':64,\n",
        "    'train_percentage':90\n",
        "}"
      ],
      "metadata": {
        "id": "k36fPf3uQAVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "Note that the dataset is taken from https://www.kaggle.com/datasets/yasserh/twitter-tweets-sentiment-dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Exploration"
      ],
      "metadata": {
        "id": "3we-T_9YTA8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Tweets.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "I8sHxxHgUo3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'].unique()"
      ],
      "metadata": {
        "id": "oWJHj0UCWYjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.loc[df['sentiment']!='neutral']"
      ],
      "metadata": {
        "id": "67uxuBMfYFNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df =df[['text','sentiment']]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Iq4lAfc4ZE0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'].count()"
      ],
      "metadata": {
        "id": "JRgPBOa1ZYs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing\n",
        "\n",
        "\n",
        "\n",
        "# Lemmatization"
      ],
      "metadata": {
        "id": "VWtpjthlag6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "5ovJw_9Aamcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"settings\")"
      ],
      "metadata": {
        "id": "wfL-h7kJeKNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopword Removal"
      ],
      "metadata": {
        "id": "jweYqJRqeaT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en = spacy.load('en_core_web_sm')\n",
        "stopwords = en.Defaults.stop_words"
      ],
      "metadata": {
        "id": "LbCzV3D1ebXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(stopwords)"
      ],
      "metadata": {
        "id": "pIIjEeR6fDkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopwords)"
      ],
      "metadata": {
        "id": "JLr0idpefayH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(stopwords)[:10]"
      ],
      "metadata": {
        "id": "z-BCGPHofeWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Pipeline"
      ],
      "metadata": {
        "id": "SddmK55Ofx3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(txt):\n",
        "    text = txt.lower()\n",
        "    tokens = txt.split()\n",
        "    tokens = [lemmatizer.lemmatize(token)for token in tokens]\n",
        "    txt = ''.join(tokens)\n",
        "    txt =txt.translate(str.maketrans('','',string.punctuation))\n",
        "    tokens = txt.split()\n",
        "    tokens=[token for token in tokens if token not in stopwords]\n",
        "    txt = ''.join(tokens)\n",
        "    txt =re.sub('r[0-9]+','',txt)\n",
        "    return txt"
      ],
      "metadata": {
        "id": "hviRzgL2fzBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.reset_index()  # restore the indices of the dataframe so that it star"
      ],
      "metadata": {
        "id": "0CtuFrUlkK5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_txt = df['text'][50]\n",
        "processed_txt = preprocess(df['text'][50])\n",
        "print(f'The original text was:\\n{original_txt}\\n The preprocessed txt is :\\n{processed_txt} ')"
      ],
      "metadata": {
        "id": "w0A2jO3Zkj3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['preprocessed_text'] = df['text'].apply(lambda x: preprocess(str(x)))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "a9ZEXm1SqYht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df['preprocessed_text'].tolist()"
      ],
      "metadata": {
        "id": "tymZj2-4rdZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorizer = CountVectorizer(max_features=config['max_features'])\n",
        "# features = vectorizer.fit_transform(texts)\n",
        "vectorizer =TfidfVectorizer(max_features=config['max_features'])\n",
        "features = vectorizer.fit_transform(texts)"
      ],
      "metadata": {
        "id": "zCz46Kh3r-il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "P24h7LBWs_pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_features= features.toarray()"
      ],
      "metadata": {
        "id": "YqhM2nTDuff8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np_features.shape"
      ],
      "metadata": {
        "id": "dtbYCBJlvTMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should also work with the labels..."
      ],
      "metadata": {
        "id": "YEPsM0utvfSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['num_sentiment'] = df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "0H0iPFwivgHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df['num_sentiment'].tolist()"
      ],
      "metadata": {
        "id": "4gGwYBv9xgg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XigNcjUTw67F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels[:5]"
      ],
      "metadata": {
        "id": "g4HfusrYx3No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train / Test / Dev Split"
      ],
      "metadata": {
        "id": "5sUIIbl0yFIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f_train, f_rem, l_train, l_rem = train_test_split(np_features, labels, test_size=1-config['train_percentage']/100, random_state=50)\n",
        "f_test, f_dev, l_test, l_dev = train_test_split(f_rem, l_rem, test_size=0.5,random_state=50)"
      ],
      "metadata": {
        "id": "M1B4sh8M26Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'train features: {f_train.shape}, dev features: {f_dev.shape}, test features :{f_test.shape}')"
      ],
      "metadata": {
        "id": "7rpzFv5TPInk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'train labels: {len(l_train)}, dev labels: {len(l_dev)}, test labels: {len(l_test)}')"
      ],
      "metadata": {
        "id": "MBP8a5UfZz-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting Everything to Tensors\n",
        "\n",
        "The numpy array we defined above should be converted to a tensor. This tensor will be used in a \"Dataset\" object."
      ],
      "metadata": {
        "id": "4DP8xA9hclmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyVectorDataset(Dataset):\n",
        "    def __init__(self,features,labels):\n",
        "        self.features = features\n",
        "        self.labels = np.array(labels).reshape(-1, 1)\n",
        "    def __len__(self):\n",
        "        return  self.features.shape[0]\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.Tensor(self.features[idx]),torch.Tensor(self.labels[idx])\n",
        "    \n",
        "          \n",
        "        "
      ],
      "metadata": {
        "id": "HqPXNpSWcu1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=MyVectorDataset(f_train, l_train)\n",
        "test_dataset=MyVectorDataset(f_test,l_test)\n",
        "dev_dataset=MyVectorDataset(f_dev, l_dev)"
      ],
      "metadata": {
        "id": "QBhEsdhLgWs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader= DataLoader(train_dataset,batch_size=config['batch_size'],shuffle=True)\n",
        "test_dataloader= DataLoader(test_dataset,batch_size=config['batch_size'],shuffle=True)\n",
        "dev_dataloader= DataLoader(dev_dataset,batch_size=config['batch_size'],shuffle=True)"
      ],
      "metadata": {
        "id": "60R4Uqwgh4x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Neural Net Architecture"
      ],
      "metadata": {
        "id": "3paYzUUMmvxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "CRpbMKHem1Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class my_neural_net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(my_neural_net, self).__init__() \n",
        "        self.first_layer = torch.nn.Sequential( \n",
        "            nn.Linear(config['max_features'],1),\n",
        "            nn.Sigmoid()    \n",
        "        )\n",
        "    def forward(self, x):\n",
        "        output = self.first_layer(x)\n",
        "        return output   "
      ],
      "metadata": {
        "id": "SHHa_FvrnpCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_nn = my_neural_net()"
      ],
      "metadata": {
        "id": "8oj-QxQgtDGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_nn = simple_nn.to(device)"
      ],
      "metadata": {
        "id": "4j_RjDZduMbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_nn(train_dataset[:2][0])"
      ],
      "metadata": {
        "id": "uWprFe0svUFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_nn(train_dataset[:2][0]).shape"
      ],
      "metadata": {
        "id": "6Y0DSE8bvdM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "\n",
        "# Binary Cross-Entropy"
      ],
      "metadata": {
        "id": "fXqQ9JntyVk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCELoss()"
      ],
      "metadata": {
        "id": "OAjrQISOycDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer\n",
        "Note that stochastic gradient descent performs a parameter update for each training example Xi and yi label \n",
        "\n"
      ],
      "metadata": {
        "id": "AJnXJodGyznd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(simple_nn.parameters(),lr=config['learning_rate'])"
      ],
      "metadata": {
        "id": "StgfwlyLy9BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def output_to_label(out):\n",
        "    dis_to_0 = abs(out)\n",
        "    dis_to_1 = abs(out-1)\n",
        "    if dis_to_0 <= dis_to_1:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "kQ9UN3x90e1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, epoch_num):\n",
        "    num_points = len(dataloader.dataset)\n",
        "    for batch, (features, labels) in enumerate(dataloader):        \n",
        "        # Compute prediction and loss\n",
        "        pred = model(features)\n",
        "        loss = loss_fn(pred, labels)\n",
        "        \n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad() # sets gradients of all model parameters to zero\n",
        "        loss.backward() # calculate the gradients again\n",
        "        optimizer.step() # w = w - learning_rate * grad(loss)_with_respect_to_w\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(features)\n",
        "            print(f\"\\r Epoch{epoch_num}_loss:{loss:7f}[{current:>5d}/{num_points:>5d}]\",end=\"\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn, epoch_num, name):\n",
        "    num_points = len(dataloader.dataset)\n",
        "    sum_test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (features, labels) in enumerate(dataloader):\n",
        "            pred = model(features)\n",
        "            sum_test_loss += loss_fn(pred, labels).item() # add the current loss to the sum of the losses\n",
        "            # convert the outputs of the model on the current batch to a numpy array\n",
        "            pred_lst = list(pred.numpy().squeeze())\n",
        "            pred_lst = [output_to_label(item) for item in pred_lst]\n",
        "            # convert the original labels corresponding to the current batch to a numpy array\n",
        "            output_lst = list(labels.numpy().squeeze())\n",
        "            match_lst = [1 if p==o else 0 for (p, o) in zip(pred_lst, output_lst)]\n",
        "            # many points are labeled correctly in this batch and add the number to the overall count of the correct labeled points\n",
        "            correct += sum(match_lst)\n",
        "    sum_test_loss /= num_points\n",
        "    correct /= num_points   \n",
        "    print(f\"/r Epoch {epoch_num} - {name} Error: Accuracy:{(100*correct):>0.1f}%, Avg loss: {sum_test_loss:>8f}\",end=\"\")\n"
      ],
      "metadata": {
        "id": "RRJko4Al1KzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch_num in range(1, config['num_epochs']+1):\n",
        "    train_loop(train_dataloader, simple_nn, loss_fn, optimizer, epoch_num)\n",
        "    test_loop(dev_dataloader, simple_nn, loss_fn, epoch_num, 'Development/Validation')"
      ],
      "metadata": {
        "id": "dQ4JWV1NrRNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# saving the model "
      ],
      "metadata": {
        "id": "UpxLH0MnygKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(simple_nn.state_dict(),\"neural_net.path\")"
      ],
      "metadata": {
        "id": "q9J9TXOnynuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the model"
      ],
      "metadata": {
        "id": "F3Rsn6mYy_MC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = my_neural_net()\n",
        "model.load_state_dict(torch.load(\"neural_net.path\"))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "OZ1GKtXezFvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(test_dataset[:2][0])"
      ],
      "metadata": {
        "id": "zPFczTY-0COL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l_test[:2]"
      ],
      "metadata": {
        "id": "ExkRMUyP1ZuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The ONNX Format\n",
        "\n",
        "This format is useful when you want to use your model while coding in Java, Javascript, and C#!"
      ],
      "metadata": {
        "id": "rjjvEIaf1joC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Model"
      ],
      "metadata": {
        "id": "mvFtz8CY1tsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.zeros((1,config['max_features']))"
      ],
      "metadata": {
        "id": "Xn-QFRPB1u6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onnx.export(model,dummy_input,'neural_net.onnx')"
      ],
      "metadata": {
        "id": "GBlsF4xW2qkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "IItDggzW3BzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session = onnxruntime.InferenceSession('neural_net.onnx',None)"
      ],
      "metadata": {
        "id": "54rsH7HC3Cva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_name = session.get_inputs()[0].name\n",
        "output_name = session.get_outputs()[0].name"
      ],
      "metadata": {
        "id": "BioaiRyx30WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_name"
      ],
      "metadata": {
        "id": "OQhqoXn95KkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_name"
      ],
      "metadata": {
        "id": "sEWEOLhh5eNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = session.run([output_name], {input_name: test_dataset[0][0].numpy().reshape(1,-1)})"
      ],
      "metadata": {
        "id": "yAg_u1rB5iji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "9YYkkIIz6uMC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}